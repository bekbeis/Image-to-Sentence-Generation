{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# SETUP\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import efficientnet\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "seed = 11\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n"
      ],
      "metadata": {
        "id": "vWYH3n5T8_sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD THE DATASET\n",
        "\n",
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "!unzip -qq Flickr8k_Dataset.zip\n",
        "!unzip -qq Flickr8k_text.zip\n",
        "!rm Flickr8k_Dataset.zip Flickr8k_text.zip\n",
        "!rm readme.txt Flickr_8k.trainImages.txt Flickr_8k.testImages.txt Flickr_8k.devImages.txt\n",
        "!rm Flickr8k.lemma.token.txt ExpertAnnotations.txt CrowdFlowerAnnotations.txt\n",
        "!rm -rf __MACOSX"
      ],
      "metadata": {
        "id": "UfoxtPxL95zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SETTING CONSTANTS\n",
        "\n",
        "# Path to the images\n",
        "IMAGES_PATH = \"Flicker8k_Dataset\"\n",
        "\n",
        "# Desired image dimensions\n",
        "IMAGE_SIZE = (299, 299)\n",
        "\n",
        "# Vocabulary size\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "SEQ_LENGTH = 25\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "EMBED_DIM = 512\n",
        "\n",
        "# Per-layer units in the feed-forward network\n",
        "FF_DIM = 512\n",
        "\n",
        "# Other training parameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n"
      ],
      "metadata": {
        "id": "Buk73JeK-vXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPARATION OF THE DATASET\n",
        "\n",
        "\n",
        "def load_captions_data(filename):\n",
        "    \"\"\"Loads captions (text) data and maps them to corresponding images.\n",
        "\n",
        "    Args:\n",
        "        filename: Path to the text file containing caption data.\n",
        "\n",
        "    Returns:\n",
        "        caption_mapping: Dictionary mapping image names and the corresponding captions\n",
        "        text_data: List containing all the available captions\n",
        "    \"\"\"\n",
        "    with open(filename) as caption_file:\n",
        "        caption_data = caption_file.readlines()\n",
        "        caption_mapping = {}\n",
        "        text_data = []\n",
        "        images_to_skip = set()\n",
        "\n",
        "        for line in caption_data:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "\n",
        "            # Image name and captions are separated using a tab\n",
        "            img_name, caption = line.split(\"\\t\")\n",
        "\n",
        "            # Each image is repeated five times for the five different captions.\n",
        "            # Each image name has a suffix `#(caption_number)`\n",
        "            img_name = img_name.split(\"#\")[0]\n",
        "            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n",
        "\n",
        "            # Remove captions that are either too short or too long\n",
        "            tokens = caption.strip().split()\n",
        "\n",
        "            if len(tokens) < 5 or len(tokens) > SEQ_LENGTH:\n",
        "                images_to_skip.add(img_name)\n",
        "                continue\n",
        "\n",
        "            if img_name.endswith(\"jpg\") and img_name not in images_to_skip:\n",
        "                # Add a start and an end token to each caption\n",
        "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
        "                text_data.append(caption)\n",
        "\n",
        "                if img_name in caption_mapping:\n",
        "                    caption_mapping[img_name].append(caption)\n",
        "                else:\n",
        "                    caption_mapping[img_name] = [caption]\n",
        "\n",
        "        for img_name in images_to_skip:\n",
        "            if img_name in caption_mapping:\n",
        "                del caption_mapping[img_name]\n",
        "\n",
        "        return caption_mapping, text_data\n",
        "\n",
        "\n",
        "def split_captions_data(caption_data, train_size=0.8, val_size=0.1, shuffle=True):\n",
        "    \"\"\"Split the captioning dataset into train, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        caption_data (dict): Dictionary containing the mapped caption data\n",
        "        train_size (float): Fraction of all the full dataset to use as training data\n",
        "        val_size (float): Fraction of all the full dataset to use as validation data\n",
        "        shuffle (bool): Whether to shuffle the dataset before splitting\n",
        "\n",
        "    Returns:\n",
        "        Traning, validation and test datasets as three separated dicts\n",
        "    \"\"\"\n",
        "    # 1. Get the list of all image names\n",
        "    all_images = list(caption_data.keys())\n",
        "\n",
        "    # 2. Shuffle if necessary\n",
        "    if shuffle:\n",
        "        np.random.shuffle(all_images)\n",
        "\n",
        "    # 3. Split into training, validation, and test sets\n",
        "    train_stop = int(len(caption_data) * train_size)\n",
        "    training_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[:train_stop]\n",
        "    }\n",
        "    val_stop = int(len(caption_data) * (train_size + val_size))\n",
        "    validation_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[train_stop:val_stop]\n",
        "    }\n",
        "    testing_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[val_stop:]\n",
        "    }\n",
        "\n",
        "    # 4. Return the splits\n",
        "    return training_data, validation_data, testing_data\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "captions_mapping, text_data = load_captions_data(\"Flickr8k.token.txt\")\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, valid_data, test_data = split_captions_data(captions_mapping)\n",
        "print(\"The number of training samples: \", len(train_data))\n",
        "print(\"The number of validation samples: \", len(valid_data))\n",
        "print(\"The number of test samples: \", len(test_data))\n"
      ],
      "metadata": {
        "id": "OekU9hdGH6L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd19e059-0db0-41c8-9e28-1f78baeeb4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of training samples:  6114\n",
            "The number of validation samples:  764\n",
            "The number of test samples:  765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VECTORIZING THE TEXT DATA\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "strip_chars = strip_chars.replace(\"<\", \"\")\n",
        "strip_chars = strip_chars.replace(\">\", \"\")\n",
        "\n",
        "vectorization = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LENGTH,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "vectorization.adapt(text_data)\n",
        "\n",
        "# Data augmentation for image data\n",
        "image_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.2),\n",
        "        layers.RandomContrast(0.3),\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "U-hE3BWsNwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BUILDING A tf.data.Dataset PIPELINE FOR TRAINING\n",
        "# i.e., generating pairs of images and corresponding captions using a tf.data.Dataset object\n",
        "\n",
        "\n",
        "def decode_and_resize(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_input(img_path, captions):\n",
        "    return decode_and_resize(img_path), vectorization(captions)\n",
        "\n",
        "\n",
        "def make_dataset(images, captions):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n",
        "    dataset = dataset.shuffle(BATCH_SIZE * 8)\n",
        "    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Pass the list of images and the list of corresponding captions\n",
        "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n",
        "valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))\n"
      ],
      "metadata": {
        "id": "NUOayTqFY14l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BUILDING THE MODEL (Hybrid CNN-Transformer)\n",
        "#\n",
        "# The architecture consists of 3 models:\n",
        "# 1. A CNN: to extract the image features\n",
        "# 2. A Transformer encoder: to generate a new representation of the inputs\n",
        "#                           from the extracted image features\n",
        "# 3. A Transformer decoder: to learn to generate the caption based on the\n",
        "#                           encoder output and the text data (sequences)\n",
        "\n",
        "\n",
        "def get_cnn_model():\n",
        "    base_model = efficientnet.EfficientNetB0(\n",
        "        input_shape=(*IMAGE_SIZE, 3),\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\",\n",
        "    )\n",
        "    # Freeze the feature extractor\n",
        "    # i.e., the weights of the base model won't be updated during training\n",
        "    base_model.trainable = False\n",
        "\n",
        "    base_model_out = base_model.output\n",
        "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
        "\n",
        "    # Create a new Keras model using the input of the base model and the reshaped output\n",
        "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
        "    return cnn_model\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        # Create instances of Keras layers\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n",
        "\n",
        "    def call(self, inputs, training, mask=None):\n",
        "        # Apply layer normalization and pass the normalized input\n",
        "        # through the dense layer\n",
        "        inputs = self.layernorm_1(inputs)\n",
        "        inputs = self.dense_1(inputs)\n",
        "\n",
        "        # Multi-head self-attention\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=None,\n",
        "            training=training,\n",
        "        )\n",
        "        # Normalization\n",
        "        out_1 = self.layernorm_2(inputs + attention_output_1)\n",
        "        return out_1\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Two embedding layers are defined:\n",
        "        #     self.token_embeddings for token embeddings\n",
        "        #     self.position_embeddings for positional embeddings.\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        # Scaling factor used to scale the embeddings\n",
        "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        # Compute the positional indices using tf.range\n",
        "        # based on the length of the input sequence\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_tokens = embedded_tokens * self.embed_scale\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        # Compute a mask to ignore padded values\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_heads = num_heads\n",
        "        # Layer definitions\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
        "        self.ffn_layer_2 = layers.Dense(embed_dim)\n",
        "\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "        self.embedding = PositionalEmbedding(\n",
        "            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n",
        "        )\n",
        "        # This layer applies a dense transformation followed by the softmax activation\n",
        "        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
        "\n",
        "        # Create two dropout layers with dropout rates of 0.3 and 0.5\n",
        "        # During training, a fraction of the neurons in the specified layers will be\n",
        "        # randomly set to zero, and the remaining neurons will be scaled by a factor\n",
        "        # to maintain the expected sum.\n",
        "        # This helps prevent overfitting and improves the generalization of the\n",
        "        # model to unseen data.\n",
        "        self.dropout_1 = layers.Dropout(0.3)\n",
        "        self.dropout_2 = layers.Dropout(0.5)\n",
        "        self.support_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
        "        inputs = self.embedding(inputs)\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=combined_mask,\n",
        "            training=training,\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "            training=training,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        ffn_out = self.ffn_layer_1(out_2)\n",
        "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
        "        ffn_out = self.ffn_layer_2(ffn_out)\n",
        "\n",
        "        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
        "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
        "        # The final output is obtained through a softmax activation.\n",
        "        preds = self.out(ffn_out)\n",
        "        return preds\n",
        "\n",
        "    # Generates a causal (or autoregressive) attention mask,\n",
        "    # which is a key component in the self-attention mechanism of transformers.\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        # tf.range(sequence_length) generates a 1D tensor\n",
        "        # with values from 0 to sequence_length - 1.\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        # Create a boolean mask where each element (i, j) is True if i >= j\n",
        "        # and False otherwise. Then, convert the boolean values to integers\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        # Mult is a tensor that is used to repeat the mask along the batch dimension\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        # Causal attention mask that is applied during the self-attention\n",
        "        # mechanism, preventing information flow from future positions in\n",
        "        # the sequence during training.\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "class ImageCaptioningModel(keras.Model):\n",
        "    def __init__(\n",
        "        self, cnn_model, encoder, decoder, num_captions_per_image=5, image_aug=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
        "        self.num_captions_per_image = num_captions_per_image\n",
        "        self.image_aug = image_aug\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "\n",
        "    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
        "        encoder_out = self.encoder(img_embed, training=training)\n",
        "        batch_seq_inp = batch_seq[:, :-1]\n",
        "        batch_seq_true = batch_seq[:, 1:]\n",
        "        mask = tf.math.not_equal(batch_seq_true, 0)\n",
        "        batch_seq_pred = self.decoder(\n",
        "            batch_seq_inp, encoder_out, training=training, mask=mask\n",
        "        )\n",
        "        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
        "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
        "        return loss, acc\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss, batch_acc = 0, 0\n",
        "\n",
        "        if self.image_aug:\n",
        "            batch_img = self.image_aug(batch_img)\n",
        "\n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        "\n",
        "        # 2. Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and compute the loss as well as accuracy\n",
        "        # for each caption.\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss, acc = self._compute_caption_loss_and_acc(\n",
        "                    img_embed, batch_seq[:, i, :], training=True\n",
        "                )\n",
        "\n",
        "                # 3. Update loss and accuracy\n",
        "                batch_loss += loss\n",
        "                batch_acc += acc\n",
        "\n",
        "            # 4. Get the list of all the trainable weights\n",
        "            train_vars = (\n",
        "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "            )\n",
        "\n",
        "            # 5. Get the gradients\n",
        "            grads = tape.gradient(loss, train_vars)\n",
        "\n",
        "            # 6. Update the trainable weights\n",
        "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "\n",
        "        # 7. Update the trackers\n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        "\n",
        "        # 8. Return the loss and accuracy values\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    def test_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss, batch_acc = 0, 0\n",
        "\n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        "\n",
        "        # 2. Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and compute the loss as well as accuracy\n",
        "        # for each caption.\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            loss, acc = self._compute_caption_loss_and_acc(\n",
        "                img_embed, batch_seq[:, i, :], training=False\n",
        "            )\n",
        "\n",
        "            # 3. Update loss and accuracy\n",
        "            batch_loss += loss\n",
        "            batch_acc += acc\n",
        "\n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        "\n",
        "        # 4. Update the trackers\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        "\n",
        "        # 5. Return the loss and accuracy values\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # List the metrics here so the `reset_states()` can be\n",
        "        # called automatically\n",
        "        return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "\n",
        "cnn_model = get_cnn_model()\n",
        "encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)\n",
        "decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation\n",
        ")\n"
      ],
      "metadata": {
        "id": "ibW5RAFHbvsV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "566f1f17-6542-4c23-fe6e-62fb1a0bb68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL TRAINING\n",
        "\n",
        "# Define the loss function\n",
        "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction=\"none\"\n",
        ")\n",
        "\n",
        "# EarlyStopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "# Learning Rate Scheduler for the optimizer\n",
        "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
        "        super().__init__()\n",
        "        self.post_warmup_learning_rate = post_warmup_learning_rate\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        global_step = tf.cast(step, tf.float32)\n",
        "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
        "        warmup_progress = global_step / warmup_steps\n",
        "        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
        "        return tf.cond(\n",
        "            global_step < warmup_steps,\n",
        "            lambda: warmup_learning_rate,\n",
        "            lambda: self.post_warmup_learning_rate,\n",
        "        )\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"post_warmup_learning_rate\": self.post_warmup_learning_rate,\n",
        "            \"warmup_steps\": self.warmup_steps,\n",
        "        }\n",
        "\n",
        "\n",
        "# Create a learning rate schedule\n",
        "num_train_steps = len(train_dataset) * EPOCHS\n",
        "num_warmup_steps = num_train_steps // 15\n",
        "lr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n",
        "\n",
        "# Compile the model\n",
        "caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)\n",
        "\n",
        "# Fit the model\n",
        "history = caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=[early_stopping],\n",
        ")\n"
      ],
      "metadata": {
        "id": "pko5d_rl_wD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d261a955-8c26-4e62-9681-abe8c7e46626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "96/96 [==============================] - 115s 888ms/step - loss: 28.1304 - acc: 0.1307 - val_loss: 20.4156 - val_acc: 0.3134\n",
            "Epoch 2/30\n",
            "96/96 [==============================] - 68s 708ms/step - loss: 19.3518 - acc: 0.3196 - val_loss: 17.8971 - val_acc: 0.3561\n",
            "Epoch 3/30\n",
            "96/96 [==============================] - 68s 710ms/step - loss: 17.4247 - acc: 0.3547 - val_loss: 16.7720 - val_acc: 0.3763\n",
            "Epoch 4/30\n",
            "96/96 [==============================] - 68s 712ms/step - loss: 16.3005 - acc: 0.3752 - val_loss: 16.1077 - val_acc: 0.3884\n",
            "Epoch 5/30\n",
            "96/96 [==============================] - 69s 713ms/step - loss: 15.4841 - acc: 0.3908 - val_loss: 15.6660 - val_acc: 0.3970\n",
            "Epoch 6/30\n",
            "96/96 [==============================] - 67s 698ms/step - loss: 14.8416 - acc: 0.4030 - val_loss: 15.3329 - val_acc: 0.4025\n",
            "Epoch 7/30\n",
            "96/96 [==============================] - 68s 703ms/step - loss: 14.3079 - acc: 0.4126 - val_loss: 15.0960 - val_acc: 0.4095\n",
            "Epoch 8/30\n",
            "96/96 [==============================] - 68s 708ms/step - loss: 13.8324 - acc: 0.4225 - val_loss: 15.0165 - val_acc: 0.4122\n",
            "Epoch 9/30\n",
            "96/96 [==============================] - 68s 703ms/step - loss: 13.4183 - acc: 0.4315 - val_loss: 14.8728 - val_acc: 0.4133\n",
            "Epoch 10/30\n",
            "96/96 [==============================] - 67s 693ms/step - loss: 13.0382 - acc: 0.4393 - val_loss: 14.7592 - val_acc: 0.4170\n",
            "Epoch 11/30\n",
            "96/96 [==============================] - 67s 695ms/step - loss: 12.6980 - acc: 0.4478 - val_loss: 14.7883 - val_acc: 0.4169\n",
            "Epoch 12/30\n",
            "96/96 [==============================] - 67s 699ms/step - loss: 12.3812 - acc: 0.4536 - val_loss: 14.7311 - val_acc: 0.4197\n",
            "Epoch 13/30\n",
            "96/96 [==============================] - 68s 702ms/step - loss: 12.0661 - acc: 0.4608 - val_loss: 14.7876 - val_acc: 0.4176\n",
            "Epoch 14/30\n",
            "96/96 [==============================] - 67s 696ms/step - loss: 11.7781 - acc: 0.4690 - val_loss: 14.7356 - val_acc: 0.4180\n",
            "Epoch 15/30\n",
            "96/96 [==============================] - 68s 704ms/step - loss: 11.5007 - acc: 0.4764 - val_loss: 14.7861 - val_acc: 0.4185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PLOT THE TRAINING HISTORY\n",
        "\n",
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "JLB_qKc4w5xd",
        "outputId": "e55c2df4-6064-4d52-cc9d-eb70324dc465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABWpUlEQVR4nO3deXiU1d3G8e9k33eykQBhTYCA7AXUoiCLiiJYN6rSulSFKlLXvoriRsXW4orVulZxq0YRFYtsIrJjBAQChEACWVhCMknIPvP+MclAICGZZCYzJPfnuubKzDzPnDmTYubuec75HYPZbDYjIiIi4sLcnN0BERERkcYosIiIiIjLU2ARERERl6fAIiIiIi5PgUVERERcngKLiIiIuDwFFhEREXF5CiwiIiLi8jyc3QF7MJlMZGdnExgYiMFgcHZ3REREpAnMZjNFRUXExsbi5nb2MZQ2EViys7OJj493djdERESkGbKysoiLizvrOW0isAQGBgKWDxwUFOTk3oiIiEhTGI1G4uPjrd/jZ9MmAkvtZaCgoCAFFhERkXNMU6ZzaNKtiIiIuDwFFhEREXF5CiwiIiLi8trEHBYREbEvs9lMVVUV1dXVzu6KnOPc3d3x8PBocdkRBRYREamjoqKCnJwcTpw44eyuSBvh5+dHTEwMXl5ezW5DgUVERKxMJhMZGRm4u7sTGxuLl5eXCnJKs5nNZioqKjhy5AgZGRn06NGj0QJxDVFgERERq4qKCkwmE/Hx8fj5+Tm7O9IG+Pr64unpyYEDB6ioqMDHx6dZ7WjSrYiInKG5/y9YpD72+Pekf5EiIiLi8hRYRERExOUpsIiIiJymS5cuzJ8/3y5trVy5EoPBQEFBgV3aa6806VZERNqEUaNGcd5559klaGzcuBF/f/+Wd0rsRiMsZ1FYWsmLy/bwwH9/cXZXRESkhWqL4TVFhw4dtErKxSiwnIWHm4F/fr+bTzYd5FhxubO7IyLS6sxmMycqqpxyM5vNTe7ntGnTWLVqFS+88AIGgwGDwcA777yDwWDg22+/ZdCgQXh7e/Pjjz+Snp7OlVdeSVRUFAEBAQwZMoTvv/++TnunXxIyGAz8+9//5qqrrsLPz48ePXqwaNGiZv9eP/vsM/r06YO3tzddunThH//4R53jr776Kj169MDHx4eoqCiuvvpq67H//ve/JCcn4+vrS3h4OGPGjKGkpKTZfTlX6JLQWfh7e9A5zI/9x06wK7eIkd29nd0lEZFWVVpZTe/Z3znlvXc8MQ4/r6Z9Tb3wwgvs3r2bvn378sQTTwDw66+/AvDQQw/x97//na5duxIaGkpWVhaXXnopTz/9NN7e3rz33ntMnDiRtLQ0OnXq1OB7zJkzh3nz5vHcc8/x0ksvMXXqVA4cOEBYWJhNn2vz5s1cc801PP7441x77bX89NNP3HXXXYSHhzNt2jQ2bdrE3XffzX/+8x9GjBhBfn4+q1evBiAnJ4frr7+eefPmcdVVV1FUVMTq1attCnfnKgWWRiRGB7H/2Al25hgZ2T3C2d0REZF6BAcH4+XlhZ+fH9HR0QDs2rULgCeeeIJLLrnEem5YWBj9+/e3Pn7yySdJSUlh0aJFzJgxo8H3mDZtGtdffz0AzzzzDC+++CIbNmxg/PjxNvX1+eefZ/To0Tz66KMA9OzZkx07dvDcc88xbdo0MjMz8ff35/LLLycwMJDOnTszYMAAwBJYqqqqmDx5Mp07dwYgOTnZpvc/VymwNCIpJoglv+ayM6fI2V0REWl1vp7u7HhinNPe2x4GDx5c53FxcTGPP/44X3/9tTUAlJaWkpmZedZ2+vXrZ73v7+9PUFAQhw8ftrk/O3fu5Morr6zz3MiRI5k/fz7V1dVccskldO7cma5duzJ+/HjGjx9vvRTVv39/Ro8eTXJyMuPGjWPs2LFcffXVhIaG2tyPc43msDQiMSYQgF25Rif3RESk9RkMBvy8PJxys9ceRqev9rnvvvtISUnhmWeeYfXq1aSmppKcnExFRcVZ2/H09Dzjd2MymezSx1MFBgayZcsWPvzwQ2JiYpg9ezb9+/enoKAAd3d3li5dyrfffkvv3r156aWX6NWrFxkZGXbvh6tRYGlE75ggAPbkFVNZbf9/mCIiYh9eXl5UV1c3et6aNWuYNm0aV111FcnJyURHR7N//37Hd7BGUlISa9asOaNPPXv2xN3dMqrk4eHBmDFjmDdvHlu3bmX//v0sX74csASlkSNHMmfOHH7++We8vLxISUlptf47iy4JNaJjiC8B3h4Ul1eRcbSEnlGBzu6SiIjUo0uXLqxfv579+/cTEBDQ4OhHjx49+Pzzz5k4cSIGg4FHH33UISMlDfnLX/7CkCFDePLJJ7n22mtZu3YtL7/8Mq+++ioAixcvZt++fVx44YWEhobyzTffYDKZ6NWrF+vXr2fZsmWMHTuWyMhI1q9fz5EjR0hKSmq1/juLRlga4eZmIDHaElJ25uiykIiIq7rvvvtwd3end+/edOjQocE5Kc8//zyhoaGMGDGCiRMnMm7cOAYOHNhq/Rw4cCCffPIJH330EX379mX27Nk88cQTTJs2DYCQkBA+//xzLr74YpKSknjttdf48MMP6dOnD0FBQfzwww9ceuml9OzZk0ceeYR//OMfTJgwodX67ywGcxtYC2U0GgkODqawsJCgoCC7t//IF9t4f10md/y2Gw9NSLR7+yIirqKsrIyMjAwSEhLw8fFxdnekjWjo35Ut3982jbDMnTuXIUOGEBgYSGRkJJMmTSItLc16fP/+/daCPaffPv300wbbnTZt2hnn27pMzJGSauaxaIRFRETEOWwKLKtWrWL69OmsW7eOpUuXUllZydixY60V9uLj48nJyalzmzNnDgEBAY0OV40fP77O6z788MPmfyo7S4y2BBatFBIRkdPdcccdBAQE1Hu74447nN29NsOmSbdLliyp8/idd94hMjKSzZs3c+GFF+Lu7m4t2FMrJSWFa665hoCAgLO27e3tfcZrXUWvmjksecZy8ksqCPP3cnKPRETEVTzxxBPcd9999R5zxDSF9qpFq4QKCwsBGixLvHnzZlJTU3nllVcabWvlypVERkYSGhrKxRdfzFNPPUV4eHi955aXl1NefnJvH6PRsSMfAd4edA7348CxE+zKMTJCFW9FRKRGZGQkkZGRzu5Gm9fsVUImk4mZM2cycuRI+vbtW+85b775JklJSYwYMeKsbY0fP5733nuPZcuW8eyzz7Jq1SomTJjQ4Hr6uXPnEhwcbL3Fx8c392M0mXWlUK4q3oqIiLS2Zo+wTJ8+ne3bt/Pjjz/We7y0tJSFCxda90o4m+uuu856Pzk5mX79+tGtWzdWrlzJ6NGjzzj/4YcfZtasWdbHRqPR4aElKSaI737N08RbERERJ2jWCMuMGTNYvHgxK1asIC4urt5z/vvf/3LixAluuukmm9vv2rUrERER7N27t97j3t7eBAUF1bk5mibeioiIOI9NgcVsNjNjxgxSUlJYvnw5CQkJDZ775ptvcsUVV9ChQwebO3Xw4EGOHTtGTEyMza91lNoS/bvziqlSiX4REZFWZVNgmT59Ou+//z4LFy4kMDCQ3NxccnNzKS0trXPe3r17+eGHH7j11lvrbScxMdG670FxcTH3338/69atY//+/Sxbtowrr7yS7t27M26cc3YIrU9cqC/+Xu5UVJnIOFri7O6IiIi0KzYFlgULFlBYWMioUaOIiYmx3j7++OM657311lvExcUxduzYettJS0uzrjByd3dn69atXHHFFfTs2ZNbbrmFQYMGsXr1ary9vZv5sezPzc1AYs0oyw7NYxERaXO6dOnC/PnzrY8NBgNffPFFg+fXFktNTU1t0fvaq53GTJs2jUmTJjn0PRzJpkm3Ta3i/8wzz/DMM880qR1fX1++++47W7rhNInRgWw+cJxduUVc6ezOiIiIQ+Xk5BAaGmrXNqdNm0ZBQUGdIFRbdDUiQiUzzka7NdtAJfpFRNqP1ipmWl/RVTmTdmu2QVKMpRbLrhzVYhGRdsJshooS59yaOKr/+uuvExsbi8lUd0HElVdeyR//+EcA0tPTufLKK4mKiiIgIIAhQ4bw/fffn7Xd0y8JbdiwgQEDBuDj48PgwYP5+eef65xfXV3NLbfcQkJCAr6+vvTq1YsXXnjBevzxxx/n3Xff5csvv7Tum7dy5cp6LwmtWrWKoUOH4u3tTUxMDA899BBVVVXW46NGjeLuu+/mgQceICwsjOjoaB5//PEm/b5qlZeXc/fddxMZGYmPjw/nn38+GzdutB4/fvw4U6dOpUOHDvj6+tKjRw/efvttACoqKpgxYwYxMTH4+PjQuXNn5s6da9P720ojLDboVbO0OddYxvGSCkJVol9E2rrKE/BMrHPe+6/Z4OXf6Gm/+93v+POf/8yKFSustbvy8/NZsmQJ33zzDWBZ4HHppZfy9NNP4+3tzXvvvcfEiRNJS0ujU6dOjb5HcXExl19+OZdccgnvv/8+GRkZ3HPPPXXOMZlMxMXF8emnnxIeHs5PP/3E7bffTkxMDNdccw333XcfO3fuxGg0Wr/4w8LCyM7OrtPOoUOHuPTSS5k2bRrvvfceu3bt4rbbbsPHx6dOKHn33XeZNWsW69evZ+3atUybNo2RI0dyySWXNPp5AB544AE+++wz3n33XTp37sy8efMYN24ce/fuJSwsjEcffZQdO3bw7bffWkuN1C6yefHFF1m0aBGffPIJnTp1Iisri6ysrCa9b3MpsNggwNuDTmF+ZOafYGeukRHddL1RRMTZQkNDmTBhAgsXLrQGlv/+979ERERw0UUXAdC/f3/69+9vfc2TTz5JSkoKixYtYsaMGY2+x8KFCzGZTLz55pv4+PjQp08fDh48yJ133mk9x9PTkzlz5lgfJyQksHbtWj755BPrnnq+vr6Ul5ef9RLQq6++Snx8PC+//DIGg4HExESys7N58MEHmT17Nm5ulosj/fr147HHHgOgR48evPzyyyxbtqxJgaWkpIQFCxbwzjvvWDcnfuONN1i6dClvvvkm999/P5mZmQwYMIDBgwcDlknJtTIzM+nRowfnn38+BoOBzp07N/qeLaXAYqOkmEBLYMkpUmARkbbP088y0uGs926iqVOnctttt/Hqq6/i7e3NBx98wHXXXWf9ci8uLubxxx/n66+/Jicnh6qqKkpLS8nMzGxS+zt37qRfv374+PhYnxs+fPgZ573yyiu89dZbZGZmUlpaSkVFBeedd16TP0ftew0fPhyDwWB9buTIkRQXF3Pw4EHriFC/fv3qvC4mJobDhw836T3S09OprKxk5MiR1uc8PT0ZOnQoO3fuBODOO+9kypQpbNmyhbFjxzJp0iTrVjvTpk3jkksuoVevXowfP57LL7+8wZXB9qI5LDayVrzVxFsRaQ8MBstlGWfcTvnCbszEiRMxm818/fXXZGVlsXr1aqZOnWo9ft9995GSksIzzzzD6tWrSU1NJTk5mYqKCrv9qj766CPuu+8+brnlFv73v/+RmprKH/7wB7u+x6k8PT3rPDYYDGfM42mJCRMmcODAAe69916ys7MZPXq0dVfqgQMHkpGRwZNPPklpaSnXXHMNV199td3euz4KLDayrhRSiX4REZfh4+PD5MmT+eCDD/jwww/p1asXAwcOtB5fs2YN06ZN46qrriI5OZno6Gj279/f5PaTkpLYunUrZWVl1ufWrVtX55w1a9YwYsQI7rrrLgYMGED37t1JT0+vc46Xl1eDG/ue+l5r166tUwJkzZo1BAYGNrgdjq26deuGl5cXa9assT5XWVnJxo0b6d27t/W5Dh06cPPNN/P+++8zf/58Xn/9deuxoKAgrr32Wt544w0+/vhjPvvsM/Lz8+3Sv/oosNiodqWQSvSLiLiWqVOn8vXXX/PWW2/VGV0ByxyPzz//nNTUVH755RduuOEGm0YjbrjhBgwGA7fddhs7duzgm2++4e9///sZ77Fp0ya+++47du/ezaOPPlpn1Q1Y5oFs3bqVtLQ0jh49SmVl5Rnvddddd5GVlcWf//xndu3axZdffsljjz3GrFmzrJe4Wsrf358777yT+++/nyVLlrBjxw5uu+02Tpw4wS233ALA7Nmz+fLLL9m7dy+//vorixcvJikpCYDnn3+eDz/8kF27drF7924+/fRToqOjCQkJsUv/6qPAYqP4UD+V6BcRcUEXX3wxYWFhpKWlccMNN9Q59vzzzxMaGsqIESOYOHEi48aNqzMC05iAgAC++uortm3bxoABA/i///s/nn322Trn/OlPf2Ly5Mlce+21DBs2jGPHjnHXXXfVOee2226jV69eDB48mA4dOtQZ4ajVsWNHvvnmGzZs2ED//v254447uOWWW3jkkUds+G007m9/+xtTpkzhxhtvZODAgezdu5fvvvvOWizPy8uLhx9+mH79+nHhhRfi7u7ORx99BEBgYCDz5s1j8ODBDBkyhP379/PNN9/YLVDVx2BuavlaF2Y0GgkODqawsLBVdm6e/OoatmQW8OL1A7iiv5OW+4mIOEBZWRkZGRkkJCTUmWAq0hIN/buy5ftbIyzNoIq3IiIirUuBpRlqN0HUSiEREZHWocDSDL1rJt7uVIl+ERGRVqHA0gynl+gXERERx1JgaYbaEv2geiwi0ja1gfUY4kLs8e9JgaWZEqO1c7OItD211VNPnDjh5J5IW1L77+n06ry20F5CzZQUE8T/duRppZCItCnu7u6EhIRY96Tx8/Ors6eNiC3MZjMnTpzg8OHDhISE4O7u3uy2FFiaqbbi7a5cjbCISNtSu5NwUzfSE2lMSEjIWXeobgoFlmaq3QQxLa+IqmoTHu66uiYibYPBYCAmJobIyMh6S8eL2MLT07NFIyu1FFiaqVOYH35e7pyoqGb/sRK6RwY6u0siInbl7u5uly8aEXvQsEAzubkZ6FUz8XaHJt6KiIg4lAJLCySp4q2IiEirUGBpgaTo2oq3CiwiIiKOpMDSAtYRFq0UEhERcSgFlhaoncOSU1hGwQmV6BcREXEUBZYWCPTxJD7MF9BGiCIiIo6kwNJCtfVYNI9FRETEcRRYWujkPBYFFhEREUdRYGmhkyuFdElIRETEURRYWqh2hGV3TYl+ERERsT8FlhaqLdFfXmVi/7ESZ3dHRESkTVJgaaFTS/TrspCIiIhjKLDYgVYKiYiIOJYCix30jrGMsKjirYiIiGMosNhBYoxGWERERBxJgcUOVKJfRETEsRRY7CDIx5O4UJXoFxERcRQFFjtRxVsRERHHUWCxk9qKt7s0wiIiImJ3Cix2UjvCslMjLCIiInZnU2CZO3cuQ4YMITAwkMjISCZNmkRaWlqdc0aNGoXBYKhzu+OOO87artlsZvbs2cTExODr68uYMWPYs2eP7Z/GiWpXCqXlFlFtMju5NyIiIm2LTYFl1apVTJ8+nXXr1rF06VIqKysZO3YsJSV1S9Lfdttt5OTkWG/z5s07a7vz5s3jxRdf5LXXXmP9+vX4+/szbtw4ysrKbP9ETtI5zA9fT0uJ/oyjKtEvIiJiTx62nLxkyZI6j9955x0iIyPZvHkzF154ofV5Pz8/oqOjm9Sm2Wxm/vz5PPLII1x55ZUAvPfee0RFRfHFF19w3XXX2dJFp6kt0Z+aVcCuXCPdIwOc3SUREZE2o0VzWAoLCwEICwur8/wHH3xAREQEffv25eGHH+bEiRMNtpGRkUFubi5jxoyxPhccHMywYcNYu3Ztva8pLy/HaDTWubmCJBWQExERcQibRlhOZTKZmDlzJiNHjqRv377W52+44QY6d+5MbGwsW7du5cEHHyQtLY3PP/+83nZyc3MBiIqKqvN8VFSU9djp5s6dy5w5c5rbdYdJitFKIREREUdodmCZPn0627dv58cff6zz/O233269n5ycTExMDKNHjyY9PZ1u3bo1v6enePjhh5k1a5b1sdFoJD4+3i5tt4Q2QRQREXGMZl0SmjFjBosXL2bFihXExcWd9dxhw4YBsHfv3nqP1851ycvLq/N8Xl5eg/NgvL29CQoKqnNzBYk1IyzZhWUUnqh0cm9ERETaDpsCi9lsZsaMGaSkpLB8+XISEhIafU1qaioAMTEx9R5PSEggOjqaZcuWWZ8zGo2sX7+e4cOH29I9pwvy8aRjSE2JftVjERERsRubAsv06dN5//33WbhwIYGBgeTm5pKbm0tpaSkA6enpPPnkk2zevJn9+/ezaNEibrrpJi688EL69etnbScxMZGUlBQADAYDM2fO5KmnnmLRokVs27aNm266idjYWCZNmmS/T9pKrCX6dVlIRETEbmyaw7JgwQLAUhzuVG+//TbTpk3Dy8uL77//nvnz51NSUkJ8fDxTpkzhkUceqXN+WlqadYURwAMPPEBJSQm33347BQUFnH/++SxZsgQfH59mfiznSYoJ5PudedoEUURExI4MZrP5nC/LajQaCQ4OprCw0OnzWb7ZlsNdH2yhf1wwX84436l9ERERcWW2fH9rLyE7S6zZBDEtTyX6RURE7EWBxc46h/vj6+lOWaWJ/cdUol9ERMQeFFjszN3NQM+aURbVYxEREbEPBRYH6K2KtyIiInalwOIAqngrIiJiXwosDmCtxZKrERYRERF7UGBxgF41c1gOFZSqRL+IiIgdKLA4QLDvyRL9u1SiX0REpMUUWBwkKUYrhUREROxFgcVBNI9FRETEfhRYHEQrhUREROxHgcVBai8JqUS/iIhIyymwOEjncH98PN1Uol9ERMQOFFgcxN3NQK+ay0KqeCsiItIyCiwOlKQ9hUREROxCgcWBTq4UUmARERFpCQUWB0q0jrDokpCIiEhLKLA4UGLNCMuhglIKS1WiX0REpLkUWByoTol+zWMRERFpNgUWB6utx6KKtyIiIs2nwOJgqngrIiLScgosDpZYuwmiRlhERESaTYHFwWqXNqflGlWiX0REpJkUWBysyykl+g+oRL+IiEizKLA4mLubgV5RqsciIiLSEgosraB24q0q3oqIiDSPAksrqF3arJVCIiIizaPA0gpqK97qkpCIiEjzKLC0gqRolegXERFpCQWWVhDs50lssA8AaarHIiIiYjMFllZSW49FE29FRERsp8DSShI18VZERKTZFFhaSZIm3oqIiDSbAksrqa3FkpZbpBL9IiIiNlJgaSUJEf54e7hRWllNZv4JZ3dHRETknKLA0krc3Qz0itY8FhERkeZQYGlFtfVYdimwiIiI2ESBpRXVrhTaoYm3IiIiNlFgaUWqxSIiItI8NgWWuXPnMmTIEAIDA4mMjGTSpEmkpaVZj+fn5/PnP/+ZXr164evrS6dOnbj77rspLCw8a7vTpk3DYDDUuY0fP755n8iF1V4SOni8FGOZSvSLiIg0lU2BZdWqVUyfPp1169axdOlSKisrGTt2LCUlJQBkZ2eTnZ3N3//+d7Zv384777zDkiVLuOWWWxpte/z48eTk5FhvH374YfM+kQtTiX4REZHm8bDl5CVLltR5/M477xAZGcnmzZu58MIL6du3L5999pn1eLdu3Xj66af5/e9/T1VVFR4eDb+dt7c30dHRNnb/3JMYE0R2YRk7c4wM6RLm7O6IiIicE1o0h6X2Uk9YWMNfvIWFhQQFBZ01rACsXLmSyMhIevXqxZ133smxY8caPLe8vByj0Vjndq5Ispbo1wiLiIhIUzU7sJhMJmbOnMnIkSPp27dvveccPXqUJ598kttvv/2sbY0fP5733nuPZcuW8eyzz7Jq1SomTJhAdXV1vefPnTuX4OBg6y0+Pr65H6PV1Va8VS0WERGRpjOYzeZm1Ym/8847+fbbb/nxxx+Ji4s747jRaOSSSy4hLCyMRYsW4enp2eS29+3bR7du3fj+++8ZPXr0GcfLy8spLy+v817x8fHW0RxXtvdwMWOeX4Wvpzu/zhmHm5vB2V0SERFxCqPRSHBwcJO+v5s1wjJjxgwWL17MihUr6g0rRUVFjB8/nsDAQFJSUmwKKwBdu3YlIiKCvXv31nvc29uboKCgOrdzRZdwP2uJ/gMq0S8iItIkNgUWs9nMjBkzSElJYfny5SQkJJxxjtFoZOzYsXh5ebFo0SJ8fHxs7tTBgwc5duwYMTExNr/W1Xm4u1lL9KvirYiISNPYFFimT5/O+++/z8KFCwkMDCQ3N5fc3FxKS0uBk2GlpKSEN998E6PRaD3n1PkoiYmJpKSkAFBcXMz999/PunXr2L9/P8uWLePKK6+ke/fujBs3zo4f1XUkak8hERERm9i0rHnBggUAjBo1qs7zb7/9NtOmTWPLli2sX78egO7du9c5JyMjgy5dugCQlpZmXWHk7u7O1q1beffddykoKCA2NpaxY8fy5JNP4u3t3ZzP5PKsE29Vi0VERKRJbAosjc3PHTVqVKPnnN6Or68v3333nS3dOOfVlujXCIuIiEjTaC8hJ6itxaIS/SIiIk2jwOIEIX5exKhEv4iISJMpsDhJolYKiYiINJkCi5PUzmPZoRL9IiIijVJgcZLEmsCyK1cjLCIiIo1RYHGS3jUTb9NyizCZmrU7goiISLuhwOIkXcL98fJw40RFNZkq0S8iInJWCixO4uHuRq8oVbwVERFpCgUWJ7KW6NfSZhERkbNSYHEiVbwVERFpGgUWJ0qsmXirlUIiIiJnp8DiREk1myBm5ZdSpBL9IiIiDVJgcaJQfy+ig1SiX0REpDEKLE5WuxGi5rGIiIg0TIHFyWor3mqlkIiISMMUWJxMK4VEREQap8DiZEnRKtEvIiLSGAUWJ0uIUIl+ERGRxiiwOJmHuxs9owIA1WMRERFpiAKLC6itx7IjRxNvRURE6qPA4gJqVwrt0sRbERGReimwuABrLRZdEhIREamXAosLUIl+ERGRs1NgcQEq0S8iInJ2CiwuItF6WUiBRURE5HQKLC4iMVoTb0VERBqiwOIitAmiiIhIwxRYXETtnkIq0S8iInImBRYX0TXCHy93N0oqqsk6rhL9IiIip1JgcREe7m70qCnRv1MVb0VEROpQYHEhtZeFNI9FRESkLgUWF5IYbZl4q00QRURE6lJgcSG9rSMsuiQkIiJyKgWWpjCZWuVtajdBzMw/QXF5Vau8p4iIyLlAgeVsyothyV/hoxvA7PilxmH+XkQFeQOQpstCIiIiVgosZ1OUAxvfgN3fwq+ft8pb1la81WUhERGRkxRYziaiB1xwn+X+tw9C6XGHv6VWComIiJxJgaUx58+EiF5QcgSWznb429WW6N+lTRBFRESsFFga4+ENE1+w3N/yHuz/0aFvVzvCsivHqBL9IiIiNRRYmqLzcBj8R8v9r2ZCZZnD3irhlBL9B4+XOux9REREziU2BZa5c+cyZMgQAgMDiYyMZNKkSaSlpdU5p6ysjOnTpxMeHk5AQABTpkwhLy/vrO2azWZmz55NTEwMvr6+jBkzhj179tj+aRxp9GMQEA3H9sCPzzvsbTxPKdG/Q/NYREREABsDy6pVq5g+fTrr1q1j6dKlVFZWMnbsWEpKSqzn3HvvvXz11Vd8+umnrFq1iuzsbCZPnnzWdufNm8eLL77Ia6+9xvr16/H392fcuHGUlTluJMNmviFw6TzL/dXPw+FdDnur2pVCqngrIiJiYTCbm19g5MiRI0RGRrJq1SouvPBCCgsL6dChAwsXLuTqq68GYNeuXSQlJbF27Vp+85vfnNGG2WwmNjaWv/zlL9x3n2VFTmFhIVFRUbzzzjtcd911jfbDaDQSHBxMYWEhQUFBzf04jTObLTVZ0r6B+N/AH74FN/tfVfv36n089fVOxvWJ4l83DrZ7+yIiIq7Alu/vFn3bFhYWAhAWFgbA5s2bqaysZMyYMdZzEhMT6dSpE2vXrq23jYyMDHJzc+u8Jjg4mGHDhjX4mvLycoxGY51bqzAY4NLnwCsAstbBlncc8jbWibdaKSQiIgK0ILCYTCZmzpzJyJEj6du3LwC5ubl4eXkREhJS59yoqChyc3Prbaf2+aioqCa/Zu7cuQQHB1tv8fHxzf0YtguOg9E1y5uXPgbGHLu/Re0miAeOqUS/iIgItCCwTJ8+ne3bt/PRRx/Zsz9N8vDDD1NYWGi9ZWVltW4HhtwKHQdBuRGWPGj35sMDvIkMrC3Rr1EWERGRZgWWGTNmsHjxYlasWEFcXJz1+ejoaCoqKigoKKhzfl5eHtHR0fW2Vfv86SuJzvYab29vgoKC6txalZs7THwR3Dxgx5ew6xu7v4Uq3oqIiJxkU2Axm83MmDGDlJQUli9fTkJCQp3jgwYNwtPTk2XLllmfS0tLIzMzk+HDh9fbZkJCAtHR0XVeYzQaWb9+fYOvcQnRfWHEny33v7kPyu07EpJorXirwCIiImJTYJk+fTrvv/8+CxcuJDAwkNzcXHJzcykttRQ4Cw4O5pZbbmHWrFmsWLGCzZs384c//IHhw4fXWSGUmJhISkoKAAaDgZkzZ/LUU0+xaNEitm3bxk033URsbCyTJk2y3yd1hN8+CKEJYDwEy560a9O9Y7QJooiISC0PW05esGABAKNGjarz/Ntvv820adMA+Oc//4mbmxtTpkyhvLyccePG8eqrr9Y5Py0tzbrCCOCBBx6gpKSE22+/nYKCAs4//3yWLFmCj49PMz5SK/L0hcv/Cf+ZBBteh37XQJx9liHX1mJJyy3CZDLj5mawS7siIiLnohbVYXEVrVaHpSEpd8AvH0JkH/jTKnD3bHGTldUm+sz+jopqEz/cfxGdwv3s0FERERHX0Wp1WKTG2KfBNwwO/wo/vWSXJj3d3egeaSnRv1PzWEREpJ1TYLEH/3AYP9dyf9WzcCzdLs3WTrzVSiEREWnvFFjspd+10HUUVJXB4nstZfxbqHbi7S5NvBURkXZOgcVeDAbLBFwPH8hYBb+0vKBe7cRbXRISEZH2ToHFnsK6wqiHLPe/+yuUHG1Rc0kxJ0v0l6hEv4iItGMKLPY2fAZE9YXSfPju/1rUVHiANx1qSvQv23XYHr0TERE5Jymw2Ju7p6VsPwbY+hGkL29Rc1cN6AjAw59t1b5CIiLSbimwOELcIBj2J8v9xfdCxYlmN3X/uF78pmsYJRXV3PreRvJLKuzUSRERkXOHAoujXPwIBHWE4/stS52bydPdjQVTB9EpzI+s/FLufH8zFVUm+/VTRETkHKDA4ijegXDZPyz3f3oJcrc1u6lQfy/+ffNgArw9WJ+Rz2OLttMGChSLiIg0mQKLI/WaAL2vBHM1LLobTNXNbqpnVCAvXn8eBgN8uCGLd3/ab79+ioiIuDgFFkebMA+8gyF7C2x4o0VNXZwYxcMTEgF4YvEOVu85Yo8eioiIuDwFFkcLjIZLHrfcX/YEFGS1qLnbLujK5IEdMZlh+gdb2HekuOV9FBERcXEKLK1h4DSI/w1UlsA397WobL/BYOCZq5IZ2CkEY1kVt767icITlfbrq4iIiAtSYGkNbm4w8QVw84TdS2DHly1qzsfTndduHERssA/7jpYw48MtVFVr5ZCIiLRdCiytJTIRLphluf/tA1Ba0LLmAn14/abB+Hq6s3rPUZ7+ZmfL+ygiIuKiFFha0/mzILwHFOfB94+3uLm+HYN5/pr+ALy9Zj8fbchscZsiIiKuSIGlNXn6wMT5lvub34YDa1vc5ITkGO4d0xOAR7/czvp9x1rcpoiIiKtRYGltXc6HgTdZ7n91D1SVt7jJu0d357J+MVRWm7nzgy1k5Td/KwARERFXpMDiDJc8Af6RcDQNfvxni5szGAz8/er+9O0YRH5JBbe+u4ni8io7dFRERMQ1KLA4g28oTPib5f7qf8CRtJY36eXOGzcNpkOgN2l5Rcz8KBWTSeX7RUSkbVBgcZY+k6HHWKiugK9mgqnly5Jjgn15/cZBeHm48f3OPP7+v5YHIREREVegwOIsBoNlc0RPP8j8CX5+zy7NDugUyrNTkgF4dWU6X/x8yC7tioiIOJMCizOFdIKLH7Hc/99sKMq1S7NXDYjjzlHdAHjgs638nHncLu2KiIg4iwKLsw39E8ScB+WFsOQhuzV7/9hejEmKpKLKxO3/2UxOYand2hYREWltCizO5u4BV7wIBnf4NQXSltilWTc3A/OvG0CvqECOFJVz+3ubKa2otkvbIiIirU2BxRXE9Ifhd1nuf/0XKLfPDswB3h78++bBhPp5su1QIff99xfMLdh4UURExFkUWFzFqIctc1qMB2HF03ZrNj7Mj9d+PwgPNwNfb83hpeV77da2iIhIa1FgcRVe/nBZTRG59a/Boc12a3pY13CenNQXgOeX7ubbbTl2a1tERKQ1KLC4kh5jIPl3YDbBonugutJuTV8/tBPTRnQBYNYnv/BrdqHd2hYREXE0BRZXM24u+IRA3jZY96pdm37ksiQu6BFBaWU1t727iSNFLd/HSEREpDUosLiagA4wrmYOy4q5kJ9ht6Y93N14+fqBdI3wJ7uwjD/9ZxPlVVo5JCIirk+BxRWdNxW6XABVpfD1LLDjyp5gP0/euHkwgT4ebMks4OHPt2nlkIiIuDwFFldkMMDl88HdG9KXw7ZP7dp8tw4BvHLDQNwM8PmWQ7yxep9d2xcREbE3BRZXFdEdfnu/5f6ShyBvh12bv7BnBx69vDcAc7/dxYpdh+3avoiIiD0psLiyEfdAZB84cQxeHwXrFthlV+da00Z04fqh8ZjN8OcPf2ZPXpHd2hYREbEnBRZX5uEFN30BPcZCdbllpOWDKXbbJNFgMDDnir4MTQijuLyKW97dxPGSCru0LSIiYk8KLK4uIBJu+AQu/Tt4+FjmtLw6HHZ+ZZfmvTzcWDB1IHGhvmTmn+DODzZTWW2/URwRERF7UGA5FxgMMPQ2+NMPEN0PSvPh49/DlzPssu9QeIA3/755MP5e7qzbl8+cr361Q6dFRETsx+bA8sMPPzBx4kRiY2MxGAx88cUXdY4bDIZ6b88991yDbT7++ONnnJ+YmGjzh2nzOvSCW5fByJmAAX7+D7x2Phzc1OKmE6ODmH/dAAwGeH9dJv9Zu7/FbYqIiNiLzYGlpKSE/v3788orr9R7PCcnp87trbfewmAwMGXKlLO226dPnzqv+/HHH23tWvvg4QWXzIGbv4KgODieAW+OhZXPQnVVi5q+pHcU94/rBcDjX+1gzd6j9uixiIhIi3nY+oIJEyYwYcKEBo9HR0fXefzll19y0UUX0bVr17N3xMPjjNfKWSRcAHf+CF//BbZ/Biufgb3fw+TXISyh2c3e+dtu7M4t4ovUbO76YAtfTB9JQoS/HTsuIiJiO4fOYcnLy+Prr7/mlltuafTcPXv2EBsbS9euXZk6dSqZmZkNnlteXo7RaKxza5d8Q+Hqt2DyG+AdBAc3WC4R/fxBs6vjGgwG/jalH/3jQygsreTWdzdiLLPfJowiIiLN4dDA8u677xIYGMjkyZPPet6wYcN45513WLJkCQsWLCAjI4MLLriAoqL664LMnTuX4OBg6y0+Pt4R3T939LsG7vgROo2AimL48i749GY4kd+s5nw83XnjxkFEB/mQfqSEPy/8mWqTyveLiIjzGMwt2EjGYDCQkpLCpEmT6j2emJjIJZdcwksvvWRTuwUFBXTu3Jnnn3++3tGZ8vJyystP7jRsNBqJj4+nsLCQoKAgm96rTTFVw5r5sOIZMFVBYAxMWgDdLmpWc9sOFvK7f/1EWaWJkd3D+fvv+hMT7GvfPouISLtlNBoJDg5u0ve3w0ZYVq9eTVpaGrfeeqvNrw0JCaFnz57s3bu33uPe3t4EBQXVuQng5g4X/AVuWQrh3aEoB/4zCb77P6gss7m55LhgXrhuAD6ebqzZe4zx81fz9dYc+/dbRESkEQ4LLG+++SaDBg2if//+Nr+2uLiY9PR0YmJiHNCzdqDjQEvNlsF/tDxe+zK8cXGz9iMa1year+++gOSOwRSWVjJ94Rb+8skvFGlei4iItCKbA0txcTGpqamkpqYCkJGRQWpqap1JskajkU8//bTB0ZXRo0fz8ssvWx/fd999rFq1iv379/PTTz9x1VVX4e7uzvXXX29r96SWlz9c/k+4/iPwi4DDvzZ7P6JuHQL47M4RTL+oGwYDfLblIJe+uJpN+5s3R0ZERMRWNgeWTZs2MWDAAAYMGADArFmzGDBgALNnz7ae89FHH2E2mxsMHOnp6Rw9erLGx8GDB7n++uvp1asX11xzDeHh4axbt44OHTrY2j05Xa8JcNfaM/cjMtp2acfLw437xyXy8e3D6RjiS1Z+Kdf8ay3/+F+aSvmLiIjDtWjSrauwZdJOu2U2w8Z/w/8egaoy8A2DK16EpIk2N2Usq+TxL3/l858PAdA/PoT5156nei0iImITl5h0Ky6mwf2Iptu8H1GQjyfPX3seL10/gCAfD37JKuDSF1bz4YZM2kD+FRERF6TA0t6csR/R+83ej2hi/1iWzLyQ4V3DKa2s5uHPt3H7fzZzrLi88ReLiIjYQIGlPbLjfkSxIb58cOsw/nppIp7uBpbuyGP8C6tZmXbYQZ0XEZH2SHNY2rvS4yf3IwKIGwqT/wVhZ9/7qT6/Zhcy86NU9hy2XGK6eXhnHr40CR9Pd3v2WERE2gjNYZGmq3c/oguatR9Rn9hgvvrz+Uwb0QWAd9ce4PKXfuTX7EIHdFxERNoTjbDISccPQModkPmT5XHvK+Hy+eAXZnNTK9MOc/9/t3KkqBxPdwP3je3FbRd0xc3NYN8+i4jIOUsjLNI8oZ1h2mIYPRvcPGDHl7BgBKSvsLmpUb0iWXLPBVzSO4rKajNzv93F1H+vJ7ug1AEdFxGRtk4jLFK/Q1vg89vgWM1+Tt3HwJDboMcllj2LmshsNvPxxizmfLWD0spqgnw8ePqqZCb2j3VQx0VE5Fxhy/e3Aos0rKLEUmhu01snnwvpZNmjaMBN4B/e5KYyjpYw8+NUfskqAGDygI48fmUfgnw87dxpERE5VyiwiH3l74ONb1pqtpQVWJ5z94Y+V1mK0XUcZClM14jKahMvLdvDyyv2YjJDxxBf5l93HkO62D5HRkREzn0KLOIYlaWW5c8b3oCc1JPPx5wHQ26F5KvB07fRZjYfyGfmx6lk5ZfiZoA7R3Vj5pieeLprSpWISHuiwCKOZTZb5rhsfAO2f27ZVBHAJwQG/B6G3NJoHZeiskrmfLWD/24+CEC/uGD+ee15dOsQ4ODOi4iIq1BgkdZTcgx+/g9sehMKMk8+38RJut9sy+Hhz7dRWFqJr6c7j1yexA1DO2FowiUmERE5tymwSOszVcOepZYdofcuPfl8Eybp5haW8ZdPU1mz9xgAY5Ii+duUfkQEeLdGz0VExEkUWMS5Gpqk23eyZa5LPZN0TSYzb63JYN6SNCqqTUQEePHc1f25KDGy9fsvIiKtQoFFXMPZJukOvQ36Tjljku7OHCMzP0olLa8IgBt/05m/XpqEr5f2IxIRaWsUWMS12DhJt6yymnlL0nhrTQYA3Tr487cp/bT8WUSkjVFgEddlwyTd1XuO8JdPfuFwkSXgXNAjgr+M7cV58SFO6LiIiNibAou4viZO0j1eUsG879L4dFMWVSbLP9UxSZHce0lP+sQGO6nzIiJiDwoscm5pwiTdzPxSXly+h8+3HKQmtzChbzT3XtKTnlGBTuu6iIg0nwKLnJsqTsCvn9c/SbfvFOg+mn2GTrywfC+LfsnGbLYsNprYL5aZY3rQVUXnRETOKQoscm5raJIuQGAMdLuYnIgRPL8vlk93lALgZoCrBsRxz+gedAr3c1LHRUTEFgos0naUHINtn1rmuexfA1Wlpxw0cKJDP5ZV9uW9vG78bO4Obp78bnAcMy7uQceQxvc1EhER51Fgkbapsgwyf4K9yyB9ORzeUefwCYMfq6t684OpH2s5jwuGDmL6Rd2JDPJxUodFRORsFFikfTDmWIJL+jJIXwGl+XUO7zNFs4b+ePYcw5hLpxARVv/WACIi4hwKLNL+mKotE3X3WgKMOWsDBnO19XCl2Z3s4POIHHApvoljITr5jO0BRESkdSmwiJQVYt63ipwtX+O+bwVRprw6h03+kbh1uxi6j4auF0FAByd1VESk/VJgETmF2WTip40b2Lryc3oWb2C42w78DOV1T4rpD90uhm6jIX4YeHg5p7MiIu2IAotIPUwmM0t+zeXl//1K8LEt/NZtK6M8tpHI/ronegVAlwssoy/dLobwbk7pr4hIW6fAInIW1SYzi7dmM//7PWQcLaEDBUzw38nNHdLpatyA4cTRui8I7QJdzoeOgyB2IET1AXdPp/RdRKQtUWARaYKqahOf/3yIF5ft4eBxS32X2CAvHhlcxVjvX/HYtwKy1oGpqu4LPXwsk3ZrA0zHQZbdpt3cnPApRETOXQosIjaoqDLx6eYsXl6+l5zCMgDiQn25e3QPJvcOwiPrJzi40VJ9N3sLlBWe2Yh3MHQccDLAdBwIQbGt/ElERM4tCiwizVBWWc1HGzJ5ZWU6R4osk3ITIvy5Z3QPJvaPxd3NACYTHM+AQ5stAebQZsjdClVlZzYYGFMTYGpusQPAN7SVP5WIiOtSYBFpgdKKat5fd4AFq9LJL6kAoFOYH9cP7cTvBscREeBd9wXVlZaqu7UBJvtny2Oz6czGw7qdHIHpOMhyaclTWwiISPukwCJiByXlVbzz035e/2EfhaWVAHi6GxjbJ5qpwzoxvGs4hoaKz1WUQM7WmgBTE2SO7z/zPDcPiOx9MsDEDoQOieDu4bgPJiLiIhRYROzoREUVi7fm8MH6TH7JKrA+3zXCn+uHdmLKoDjC/JtQt+VE/sl5MLWXlEoOn3mep5+lLkzHQZbLSB0HWVYqqTKviLQxCiwiDvJrdiEL12fyZWo2xeWW1UNe7m5MSI7mhqGdGJoQ1vCoy+nMZjAeqjsfJjsVKorOPNc7yFIPJrwHhHevuV/z0zvQfh9QRKQVKbCIOFhJeRWLfslm4fpMth06uWqoe2QANwztxJSBcQT7NaNWi8kEx/acEmC2QO42qK5o+DUB0aeFmJpbaBdV7BURl6bAItKKth4sYOH6TBb9ks2JCsuGi94eblzWL4apwzoxsFNo00dd6lNVDvkZcGzvKbd0y8/6LinVMrhBSOdTQswpgSaoo+rGiIjTKbCIOEFRWSVfpFpGXXbmGK3P94oK5IZhnbhqYEeCfOxcIbe0APLTTwaYUwNNRXHDr/PwsaxYOn1UJrw7+Ifbt48iIg1waGD54YcfeO6559i8eTM5OTmkpKQwadIk6/Fp06bx7rvv1nnNuHHjWLJkyVnbfeWVV3juuefIzc2lf//+vPTSSwwdOrRJfVJgEVdiNpv5Ocsy6rJ4azZllZblzT6eblzRP5YbhnWmf1xwy0ZdGu8EFOfVPyqTnwGmyoZf6xt65qhMSCdLcTwvf/AOAE9/jdCISIs5NLB8++23rFmzhkGDBjF58uR6A0teXh5vv/229Tlvb29CQxsumPXxxx9z00038dprrzFs2DDmz5/Pp59+SlpaGpGRkY32SYFFXFVhaSUpWw6ycEMmu/NOjnj0jgnihmGdmDSgIwHerbyEuboKCjPrH5UpzGp6O14BlgDjFWAJMV6BNT9rnws8eU6d47XPBZ58rQKQSLvUapeEDAZDvYGloKCAL774osntDBs2jCFDhvDyyy8DYDKZiI+P589//jMPPfRQo69XYBFXZzab2XTgOAvXZ/L1thwqqiyjLn5e7lx5Xiw3DO1Mclywk3sJVJywVPI9fVSmIMtSW6aiqP6CePbg6d9woPEOBL8ICIgE/w41PyMhoAP4hGjJt8g5ypbvb4f8X7uVK1cSGRlJaGgoF198MU899RTh4fVfF6+oqGDz5s08/PDD1ufc3NwYM2YMa9eurfc15eXllJeXWx8bjcZ6zxNxFQaDgSFdwhjSJYzZl/fms5pRl31HSvhwQxYfbsiiX1wwNwztxMT+sfi39qhLLS8/y27UUX3qP242Q2WpZX5MRTGUn/qzyBJqrM/VPG7weHHdAFRZYrmRZ1uf3b3ODDH+kfWEm0jL5S6FG5Fzkt3/Ko4fP57JkyeTkJBAeno6f/3rX5kwYQJr167F3d39jPOPHj1KdXU1UVFRdZ6Piopi165d9b7H3LlzmTNnjr27LtIqQv29uPWCrtxyfgLrM/JZuD6Tb7fnsPVgIVsPbuOpr3cyaYBl1KV3rIuNGBoMllDj5Qc0frm2UWazZR+m2vBSXnxKyCmqCUYlUGaEkiOWVVHFNbeSI1ButCz5Nh6y3Brj5mEJMQ0FnDrhJkyXqURciN0Dy3XXXWe9n5ycTL9+/ejWrRsrV65k9OjRdnmPhx9+mFmzZlkfG41G4uPj7dK2SGsxGAz8pms4v+kazrHi3vx380E+3JDJ/mMneH9dJu+vy+S8+BBuGNaJif1i8fU6M/Cf8wwGy15Knr5AB9tfX1lWE2JOCTN1Htf+zLPssm2qgqIcy63RvrmDf8TJUOMXbhmh8Q21hBnr/VDwq3nsEwxubfB/JxEX4PBx565duxIREcHevXvrDSwRERG4u7uTl1d3GDgvL4/o6Oh62/T29sbb27veYyLnovAAb/70227cdkFX1u47xsL1mXz3ay6pWQWkZhXwxFc7GJ0UyWXJMVzYswM+nvpSBMDTx7KCKaRT4+dWlVtGZWpHZ84IN6c8X5oP5mpL0CnOs+0qlU9ww4GmvsDjF6agI9IEDg8sBw8e5NixY8TExNR73MvLi0GDBrFs2TLr5F2TycSyZcuYMWOGo7sn4lLc3AyM7B7ByO4RHC4q49NNB/loYyZZ+aV8mZrNl6nZBHh7cEnvKC5NjuHCnhF4e+iLrkk8vCE4znJrTHXlmeGm9HjNLf+U+8fhRM3P2i0Vygott+MZtvXPJ/jsozduHpYRoupKy7J0U/Up96ssq79MVZbHde7XnGu9X3VKO6ffP/21p9ynZjTMw8cSFD18T46Oefic8tOv5nh9z/k28bW+TbscV11pmVNVVQaVJywjblWlZ/9ZWdr4OfWdW11hmS/l4X3K78Cn5rHv2Z/3POW4xynHm/r86fOuqqssn7mqvObnqbfys/+sbOi8058rPfMcN094IN22f9d2ZPMqoeLiYvbu3QvAgAEDeP7557nooosICwsjLCyMOXPmMGXKFKKjo0lPT+eBBx6gqKiIbdu2WUdFRo8ezVVXXWUNJB9//DE333wz//rXvxg6dCjz58/nk08+YdeuXWfMbamPVglJW2YyWeq6fL01h2+355BTWGY9FnhKeLlA4cW5qisthfzOCDSnPa5zvMAyD0fO5O51SrDxsXxZVpWdElBKLaNg7YF7TXgx1QQVZ31uNw+YfcyuTTp0ldCmTZu46KKLrI9r55LcfPPNLFiwgK1bt/Luu+9SUFBAbGwsY8eO5cknn6xzCSc9PZ2jR49aH1977bUcOXKE2bNnk5uby3nnnceSJUuaFFZE2jo3NwODOocyqHMoj1yWxM9Zx1m8NYdvt+WSayzj858P8fnPhyzhpU8Ul/eL4fzuHfDy0ITRVuXuaZnrEmDjXBxr0GloBKfmsanK8h5uHpYvb3ePBu7XnuNRz31Py6Un632Pmtc29jqPkxOkTw0M9Y1WVJ5o4Lyyen6eNipSXX7K76XCcisvbPBXV0ejozenj/A09LO+1/paAlR1xZmjD3VGLOp7/pTjlaeOWpzl+cpS4JSxhOryur+bU7l51jOy09BP30aO19fOaec4kUrzi5yjTCYzWzIt4eWbbTkcLjr5By3Qx4OxvaO5vF8MI7tHKLzIucFkauDyTKkl2DUUMjy829ZydbPZElBPDzJuHmcGiHN87pP2EhJpZ0wmM5szj/N1PeElyMeDsX2iuaxfDCO7KbyIiOtQYBFpx6pNZjbtz+ebbTl8sz2XI6eEl2BfT8b2jrKEl+4ReLorvIiI8yiwiAhgCS8ba8PLtlyOFp8MLyF+nozrHc2l/WIY0S1c4UVEWp0Ci4icodpkZkNGPl9vy2bJ9lyOFldYj4X4eTK+TzSXJscwXOFFRFqJAouInFW1ycz6jGN8vTWHJdtzOVZyMryE+nkyvm80lyXH8puuYXgovIiIgyiwiEiTVVWb2JCRz+JtlvCSf0p4CfP3YlyfaC5LjmFoQpgm7IqIXSmwiEizVFWbWJ+Rz+KtOSzZnsPxE5XWY4HeHlzYswOjkyIZ1SuSMH8vJ/ZURNoCBRYRabGqahNr91kuG32/M6/OnBeDAQZ2CmV0UiSjE6PoGRWAoS3VwRCRVqHAIiJ2ZTKZ+eVgAct2HmbZrsPszKlbTj4u1JfRiZFcnBTFb7qGaYsAEWkSBRYRcajsglKW7TrM8p15rEk/RkWVyXrMz8udC3pEMDoxilGJHYgM9HFiT0XElSmwiEirOVFRxZq9x1i+K49lOw/XqbIL0D8+hNGJkYxOiqR3TJAuHYmIlQKLiDiFyWTm12wjy3blsXzXYbYerLtxXXSQDxcnRTImKZIR3SLw8dSlI5H2TIFFRFxCnrGMFbsO8/3Ow/y49whllScvHfl4ujGyWwSjk6K4ODGS6GBdOhJpbxRYRMTllFVWs3bfMZbtzGP5zsNkF5bVOd63YxAXJ0YxOjGS5I7BuLnp0pFIW6fAIiIuzWw2szOnyDLvZddhUrMKOPUvUYdAby7uFcnFSZFc0CMCPy8P53VWRBxGgUVEzilHispZmXaY5bsO88PuI5RUVFuPeXm4MbxrOBf0iGB4t3CSooM0+iLSRiiwiMg5q7yqmg0Z+TU1X/LIyi+tczzM34vhXcMZ3i2ckd0j6BLup5VHIucoBRYRaRPMZjN7DxezMu0Ia9KPsiEjnxOnjL4AxAT7MKJbBCO6hTOiezgxwb5O6q2I2EqBRUTapMpqE79kFfBT+jHW7D3Kz5kFVFSb6pzTNcKfEd3DGdEtguFdwwnVnkciLkuBRUTahdKKajYdyOen9GP8tPco2w4VYjrlL5rBAEnRQYysCTBDEsII8NYEXhFXocAiIu1SYWkl6/cdswSY9KPsziuuc9zDzUD/+BBGdgtneLcIBnYO0b5HIk6kwCIiAhwuKmNt+jHWph9jTfrRMybwenu4MaRLmPUSUt/YIDzc3ZzUW5H2R4FFRKQeWfkn+Cn9KGv2WkZhjhbX3fco0MeDYQnh1ktIPaMCtAJJxIEUWEREGmE2m9lzuJif9h5lTfox1u07RlFZVZ1zIgK8GF6zAuk3XcO1hFrEzhRYRERsVG0ys/1QoXX+y8b9+XX2PgKICPBmaEIoQ7qEMaRLGEkxQbiriJ1IsymwiIi0UHlVNT9nWpZQr00/yi9ZhWcsoQ709mBg51CGJlgCTL+4YO1ALWIDBRYRETsrq6xm68FCNmQcY8P+42w5cJzi8rqXkLzc3egfH2wZgUkIY1DnUIJ8PJ3UYxHXp8AiIuJg1SYzO3OMbMjIZ+N+y+1ocUWdc9wMkBgdZB2BGZIQSmSgj5N6LOJ6FFhERFqZ2Wwm42gJG/fnsyHjOBv355OZf+KM87qE+1lHYIZ2CaOzJvJKO6bAIiLiAvKMZdYRmA0Z+aTlFXH6X9zIQO+aSbyhDEkIIzFaE3ml/VBgERFxQYWllWw+cHIEZuvBAiqr6/4JDvTxYFBny0qkoQmWibyqxittlQKLiMg5oKyymtSsAjZm5LNhfz5bDhyn5LTdqL083DgvLoRBXUI5Lz6E8+JDiArSPBhpGxRYRETOQVXVJnbmFLFhfz4bay4lHSupOOO8mGAfzosPoX9NgEnuGIy/NnWUc5ACi4hIG2A2m9l3tISNGfn8nFnALwcL2J1XVGdHarCsRuoZFWgdgekfH0LPqEDNhRGXp8AiItJGFZdXse1gIb8cLCC1JsTkFJadcZ6flzvJHYOtIea8TiFEB/loRZK4FAUWEZF2JM9YZh2BSc0sYOvBgjPmwoBlRVJteDkvLoTkuGACVdhOnEiBRUSkHas2mUk/UkxqZgGpNSEmLa+I6tOuJRkM0L1DgDXE9I8LITE6EA93Nyf1XNobBRYREamjtKKa7dmFdULMoYLSM87z8XQjuWMw/eNqRmLiQ+gY4qtLSeIQCiwiItKoI0Xl/JJVQGpWzeWkrAKKyqrOOC8iwJv+ccH07RhMcsdgkuOCtbRa7MKhgeWHH37gueeeY/PmzeTk5JCSksKkSZMAqKys5JFHHuGbb75h3759BAcHM2bMGP72t78RGxvbYJuPP/44c+bMqfNcr1692LVrV5P6pMAiItJyJpNlVVJtiEnNKmBnjpGq05clAR0CvekbG0Ryx5ogExesSb1iM1u+v21euF9SUkL//v354x//yOTJk+scO3HiBFu2bOHRRx+lf//+HD9+nHvuuYcrrriCTZs2nbXdPn368P3335/smIdqCoiItCY3NwPdIwPoHhnAlEFxgKW43a/ZRrYdLGDbISPbDxWy53ARR4rKWZF2hBVpR6yvjwjwso7C9Im1hJjYYIUYsQ+bU8GECROYMGFCvceCg4NZunRpnedefvllhg4dSmZmJp06dWq4Ix4eREdH29odERFxIB9PdwZ1DmVQ51Drc6UV1ezIsYSXbYcKa0JMMUeLK1iZdoSVp4SYMP/aEHNyNEZzYqQ5HD6MUVhYiMFgICQk5Kzn7dmzh9jYWHx8fBg+fDhz585tMOCUl5dTXl5ufWw0Gu3ZZREROQtfrzNDTFllNTtPCTHbDhnZk1dEfkkFP+w+wg+7T4aYUD9P+nY8ZU5Mx2DiQhVi5OxaNOnWYDDUmcNyurKyMkaOHEliYiIffPBBg+18++23FBcX06tXL3JycpgzZw6HDh1i+/btBAYGnnF+fXNeAM1hERFxIWWV1ezKLbKMwhy0BJndeUX1zokJ8fOkb2xwTZCxjMZ0CvNTiGnjWm2V0NkCS2VlJVOmTOHgwYOsXLnSpiBRUFBA586def7557nlllvOOF7fCEt8fLwCi4iIiyurrGZ3XpH1UtK2Q4Wk5RadsWs1QJCPh3UUpnZEpnOYH27acqDNcOik26aorKzkmmuu4cCBAyxfvtzmEBESEkLPnj3Zu3dvvce9vb3x9va2R1dFRKQV+Xi60y8uhH5xIdbnyquq2Z1bXHMpyRJk0nKLMJZV8VP6MX5KP2Y9N9Dbgz4dg+hbM6m3T2wwXSP8FWLaAbsHltqwsmfPHlasWEF4eLjNbRQXF5Oens6NN95o7+6JiIiL8fZwJznOEkBqVVSZ2J1XVGdi787cIorKq1i3L591+/Kt5/p7udMnNpg+p0zs7dYhQJs/tjE2B5bi4uI6Ix8ZGRmkpqYSFhZGTEwMV199NVu2bGHx4sVUV1eTm5sLQFhYGF5eXgCMHj2aq666ihkzZgBw3333MXHiRDp37kx2djaPPfYY7u7uXH/99fb4jCIico7x8nCzXga6rua5ymoTe/KK2X6okO3ZliCzI9tISUU1G/bns2H/yRDj6+lO79gg+sYGWevEdO8QoG0HzmE2B5ZNmzZx0UUXWR/PmjULgJtvvpnHH3+cRYsWAXDeeefVed2KFSsYNWoUAOnp6Rw9etR67ODBg1x//fUcO3aMDh06cP7557Nu3To6dOhga/dERKSN8nR3o3dsEL1jg7iGeACqqk2kHymxjsJsP1TIr9lGSiur2XzgOJsPHLe+3tvDjaSYIOuk3j6xwfSMCsTLQyHmXKDS/CIi0qZUm8xkHC2uCTFG60hMcfmZ2w54ubuRGBNoKXRXs0KpV3Qg3h7uTuh5+6O9hERERE5hMpnZf8wyEmOp3Gu5rFTf3kme7gZ6RgVallnHBdM3NoikmCB8PBVi7E2BRUREpBFms5nM/BPWkZjaCb6FpZVnnOvuZqBrhD9JMUEkxgSSFG35qf2TWkaBRUREpBnMZjMHj5eeMrHXEmTySyrqPT/Ez5PE6EASo4PoXRNmekYFajSmiRRYRERE7MRsNpNTWEZabhE7cozsyi1iV46RfUdLqK6naq+bAbrUjMYk1YSZxJhA7aFUDwUWERERByurrGbv4WJ21oaYXCM7c4oaHI0J9PGwXkpKjA4iKSaQXtGB+Hk5fFs/l6XAIiIi4gRms5kjReXsrBmFqQ0zew8X17uHksEAncP8LHNjok/Oj4kL9W0X1XsVWERERFxIRZWJ9CMnR2Nqfx4pKq/3/ABvD3pFB5IYHWi5tFQzNybQx7OVe+5YCiwiIiLngKPF5ezKOXk5aWeOkb2Hi6moNtV7fscQX3pFWy4l9Yqy/Ozawf+crRujwCIiInKOqqw2kXG0hJ05lhCzK9fIrpwico1l9Z7v4WYgIcKfntGBJEYFWn5GBxIf6vo7WyuwiIiItDEFJyrYnVdMWq6RtLwi0nKL2JVbVG/xO7Dsp9QzKoCeNSMxidFB9IwOoEOAt8usVlJgERERaQfMZjO5xjJ25RaxO9cSYtLyithzuJiKqvovK4X5e9EzKsASYGrCTM+oAKfMj1FgERERaceqqk0cyD9hHYXZXRNk9h8roaFv/frmx3TrEODQzSEVWEREROQMpRWW2jGWS0qWlUq784rIM9a/Wql2fkxtiLn1gq74etlvgq8Ci4iIiDTZ8ZIKducVWefG1F5aOnV+jJeHGzvmjMPD3X4jLrZ8f7ff8noiIiICQKi/F8O6hjOsa7j1uVO3JLCEl0q7hhVbKbCIiIjIGQwGA7EhvsSG+HJRYqSzu4PzopKIiIhIEymwiIiIiMtTYBERERGXp8AiIiIiLk+BRURERFyeAouIiIi4PAUWERERcXkKLCIiIuLyFFhERETE5SmwiIiIiMtTYBERERGXp8AiIiIiLk+BRURERFxem9it2Ww2A2A0Gp3cExEREWmq2u/t2u/xs2kTgaWoqAiA+Ph4J/dEREREbFVUVERwcPBZzzGYmxJrXJzJZCI7O5vAwEAMBoNd2zYajcTHx5OVlUVQUJBd2z4XtPfPD/odtPfPD/odtPfPD/odOOrzm81mioqKiI2Nxc3t7LNU2sQIi5ubG3FxcQ59j6CgoHb5j7RWe//8oN9Be//8oN9Be//8oN+BIz5/YyMrtTTpVkRERFyeAouIiIi4PAWWRnh7e/PYY4/h7e3t7K44RXv//KDfQXv//KDfQXv//KDfgSt8/jYx6VZERETaNo2wiIiIiMtTYBERERGXp8AiIiIiLk+BRURERFyeAksjXnnlFbp06YKPjw/Dhg1jw4YNzu5Sq5g7dy5DhgwhMDCQyMhIJk2aRFpamrO75TR/+9vfMBgMzJw509ldaVWHDh3i97//PeHh4fj6+pKcnMymTZuc3a1WUV1dzaOPPkpCQgK+vr5069aNJ598skl7npyrfvjhByZOnEhsbCwGg4EvvviiznGz2czs2bOJiYnB19eXMWPGsGfPHud01gHO9vkrKyt58MEHSU5Oxt/fn9jYWG666Says7Od12EHaOzfwKnuuOMODAYD8+fPb5W+KbCcxccff8ysWbN47LHH2LJlC/3792fcuHEcPnzY2V1zuFWrVjF9+nTWrVvH0qVLqaysZOzYsZSUlDi7a61u48aN/Otf/6Jfv37O7kqrOn78OCNHjsTT05Nvv/2WHTt28I9//IPQ0FBnd61VPPvssyxYsICXX36ZnTt38uyzzzJv3jxeeuklZ3fNYUpKSujfvz+vvPJKvcfnzZvHiy++yGuvvcb69evx9/dn3LhxlJWVtXJPHeNsn//EiRNs2bKFRx99lC1btvD555+TlpbGFVdc4YSeOk5j/wZqpaSksG7dOmJjY1upZ4BZGjR06FDz9OnTrY+rq6vNsbGx5rlz5zqxV85x+PBhM2BetWqVs7vSqoqKisw9evQwL1261Pzb3/7WfM899zi7S63mwQcfNJ9//vnO7obTXHbZZeY//vGPdZ6bPHmyeerUqU7qUesCzCkpKdbHJpPJHB0dbX7uueeszxUUFJi9vb3NH374oRN66Finf/76bNiwwQyYDxw40DqdamUN/Q4OHjxo7tixo3n79u3mzp07m//5z3+2Sn80wtKAiooKNm/ezJgxY6zPubm5MWbMGNauXevEnjlHYWEhAGFhYU7uSeuaPn06l112WZ1/B+3FokWLGDx4ML/73e+IjIxkwIABvPHGG87uVqsZMWIEy5YtY/fu3QD88ssv/Pjjj0yYMMHJPXOOjIwMcnNz6/y3EBwczLBhw9rl30Sw/F00GAyEhIQ4uyutxmQyceONN3L//ffTp0+fVn3vNrH5oSMcPXqU6upqoqKi6jwfFRXFrl27nNQr5zCZTMycOZORI0fSt29fZ3en1Xz00Uds2bKFjRs3OrsrTrFv3z4WLFjArFmz+Otf/8rGjRu5++678fLy4uabb3Z29xzuoYcewmg0kpiYiLu7O9XV1Tz99NNMnTrV2V1zitzcXIB6/ybWHmtPysrKePDBB7n++uvb1WaIzz77LB4eHtx9992t/t4KLNKo6dOns337dn788Udnd6XVZGVlcc8997B06VJ8fHyc3R2nMJlMDB48mGeeeQaAAQMGsH37dl577bV2EVg++eQTPvjgAxYuXEifPn1ITU1l5syZxMbGtovPLw2rrKzkmmuuwWw2s2DBAmd3p9Vs3ryZF154gS1btmAwGFr9/XVJqAERERG4u7uTl5dX5/m8vDyio6Od1KvWN2PGDBYvXsyKFSuIi4tzdndazebNmzl8+DADBw7Ew8MDDw8PVq1axYsvvoiHhwfV1dXO7qLDxcTE0Lt37zrPJSUlkZmZ6aQeta7777+fhx56iOuuu47k5GRuvPFG7r33XubOnevsrjlF7d+99v43sTasHDhwgKVLl7ar0ZXVq1dz+PBhOnXqZP27eODAAf7yl7/QpUsXh7+/AksDvLy8GDRoEMuWLbM+ZzKZWLZsGcOHD3diz1qH2WxmxowZpKSksHz5chISEpzdpVY1evRotm3bRmpqqvU2ePBgpk6dSmpqKu7u7s7uosONHDnyjKXsu3fvpnPnzk7qUes6ceIEbm51/0S6u7tjMpmc1CPnSkhIIDo6us7fRKPRyPr169vF30Q4GVb27NnD999/T3h4uLO71KpuvPFGtm7dWufvYmxsLPfffz/fffedw99fl4TOYtasWdx8880MHjyYoUOHMn/+fEpKSvjDH/7g7K453PTp01m4cCFffvklgYGB1mvUwcHB+Pr6Orl3jhcYGHjGfB1/f3/Cw8PbzTyee++9lxEjRvDMM89wzTXXsGHDBl5//XVef/11Z3etVUycOJGnn36aTp060adPH37++Weef/55/vjHPzq7aw5TXFzM3r17rY8zMjJITU0lLCyMTp06MXPmTJ566il69OhBQkICjz76KLGxsUyaNMl5nbajs33+mJgYrr76arZs2cLixYuprq62/l0MCwvDy8vLWd22q8b+DZwe0jw9PYmOjqZXr16O71yrrEU6h7300kvmTp06mb28vMxDhw41r1u3ztldahVAvbe3337b2V1zmva2rNlsNpu/+uorc9++fc3e3t7mxMRE8+uvv+7sLrUao9Fovueee8ydOnUy+/j4mLt27Wr+v//7P3N5ebmzu+YwK1asqPe/+5tvvtlsNluWNj/66KPmqKgos7e3t3n06NHmtLQ053bajs72+TMyMhr8u7hixQpnd91uGvs3cLrWXNZsMJvbcNlGERERaRM0h0VERERcngKLiIiIuDwFFhEREXF5CiwiIiLi8hRYRERExOUpsIiIiIjLU2ARERERl6fAIiIiIi5PgUVERERcngKLiIiIuDwFFhEREXF5CiwiIiLi8v4ffDn8rlMtnhYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECK SAMPLE PREDICTIONS\n",
        "\n",
        "vocab = vectorization.get_vocabulary()\n",
        "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
        "max_decoded_sentence_length = SEQ_LENGTH - 1\n",
        "valid_images = list(valid_data.keys())\n",
        "\n",
        "\n",
        "def generate_caption(img_path=None):\n",
        "    # Use a provided image or select a random image from the validation dataset\n",
        "    sample_img = img_path if img_path else np.random.choice(valid_images)\n",
        "\n",
        "    # Read the image from the disk\n",
        "    sample_img = decode_and_resize(sample_img)\n",
        "    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "    # Pass the image to the CNN\n",
        "    img = tf.expand_dims(sample_img, 0)\n",
        "    img = caption_model.cnn_model(img)\n",
        "\n",
        "    # Pass the image features to the Transformer encoder\n",
        "    encoded_img = caption_model.encoder(img, training=False)\n",
        "\n",
        "    # Generate the caption using the Transformer decoder\n",
        "    decoded_caption = \"<start> \"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
        "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "        predictions = caption_model.decoder(\n",
        "            tokenized_caption, encoded_img, training=False, mask=mask\n",
        "        )\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = index_lookup[sampled_token_index]\n",
        "        if sampled_token == \"<end>\":\n",
        "            break\n",
        "        decoded_caption += \" \" + sampled_token\n",
        "\n",
        "    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
        "    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
        "    print(\"Predicted caption: \", decoded_caption)\n",
        "\n",
        "\n",
        "# Check predictions for a few samples\n",
        "generate_caption()\n",
        "generate_caption()\n",
        "generate_caption()\n"
      ],
      "metadata": {
        "id": "5Zsa1oouB5Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVING THE MODEL WEIGHTS\n",
        "\n",
        "caption_model.save_weights(\"cnn_tr_fl_60-20-20.h5\")\n"
      ],
      "metadata": {
        "id": "FcCTIyR1MKYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CALCULATING THE BLEU SCORE"
      ],
      "metadata": {
        "id": "jgbE_23bhjnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA PREPARATION FOR BLEU\n",
        "\n",
        "\n",
        "test_images = list(test_data.keys())\n",
        "test_captions = list(test_data.values())\n",
        "\n",
        "\n",
        "def get_candidates():\n",
        "    cand = []\n",
        "    for img in test_images:\n",
        "        # Read the image from the disk\n",
        "        img = decode_and_resize(img)\n",
        "\n",
        "        # Pass the image to the CNN\n",
        "        img = tf.expand_dims(img, 0)\n",
        "        img = caption_model.cnn_model(img)\n",
        "\n",
        "        # Pass the image features to the Transformer encoder\n",
        "        encoded_img = caption_model.encoder(img, training=False)\n",
        "\n",
        "        # Generate the caption using the Transformer decoder\n",
        "        decoded_caption = \"<start> \"\n",
        "        for i in range(max_decoded_sentence_length):\n",
        "            tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
        "            mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "            predictions = caption_model.decoder(\n",
        "                tokenized_caption, encoded_img, training=False, mask=mask\n",
        "            )\n",
        "            sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "            sampled_token = index_lookup[sampled_token_index]\n",
        "            if sampled_token == \"<end>\":\n",
        "                break\n",
        "            decoded_caption += \" \" + sampled_token\n",
        "\n",
        "        decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
        "        decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
        "        cand.append(decoded_caption.split())\n",
        "    return cand\n",
        "\n",
        "\n",
        "def get_references():\n",
        "    refs = []\n",
        "    for unit in test_captions:\n",
        "        unit_refs = []\n",
        "        for cap in unit:\n",
        "            cap = cap.lower()\n",
        "            cap = re.sub(\"[%s]\" % re.escape(strip_chars), \"\", cap)\n",
        "            cap = cap.split()[1:-1]\n",
        "            unit_refs.append(cap)\n",
        "        refs.append(unit_refs)\n",
        "    return refs\n",
        "\n",
        "\n",
        "candidates = get_candidates()\n",
        "references = get_references()\n"
      ],
      "metadata": {
        "id": "A1DVVfb3hoNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "def compute_bleu_score(references, candidates):\n",
        "    results = {}\n",
        "    results[\"BLEU_1\"] = corpus_bleu(references, candidates, weights=(1, 0, 0, 0))\n",
        "    results[\"BLEU_2\"] = corpus_bleu(references, candidates, weights=(0.5, 0.5, 0, 0))\n",
        "    results[\"BLEU_3\"] = corpus_bleu(\n",
        "        references, candidates, weights=(0.33, 0.33, 0.33, 0)\n",
        "    )\n",
        "    results[\"BLEU_4\"] = corpus_bleu(\n",
        "        references, candidates, weights=(0.25, 0.25, 0.25, 0.25)\n",
        "    )\n",
        "    return results\n",
        "\n",
        "\n",
        "bleu_score = compute_bleu_score(references, candidates)\n",
        "for bleu_type, score in bleu_score.items():\n",
        "    print(bleu_type, score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAyY6qp_2iDV",
        "outputId": "4fc5d24e-9a19-4b73-88f8-0ec37cd1801e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU_1 0.6048215403882279\n",
            "BLEU_2 0.4246210978645567\n",
            "BLEU_3 0.2920748847676351\n",
            "BLEU_4 0.19256133546644863\n"
          ]
        }
      ]
    }
  ]
}